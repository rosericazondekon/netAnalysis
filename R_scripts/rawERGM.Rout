
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #!/usr/bin/env Rscript
> 
> #' This project is part of my PhD dissertation project.
> #' This section follows from the first section dedicated to the Mathematical Modeling of our co-authorship network (available [HERE](http://#)).
> #'
> #' The purpose of this section is the use of Exponential Random Graph Modeling (ERGM) as a  Statistical modeling to better understand and explain our malaria co-authorship network.
> #' Loading the necessary packages...
> 
> #+ setup, include=FALSE,cache=FALSE
> setwd('~/R/R_scripts')
> library(igraph)

Attaching package: ‘igraph’

The following objects are masked from ‘package:stats’:

    decompose, spectrum

The following object is masked from ‘package:base’:

    union

> library(ergm)
Loading required package: statnet.common
Loading required package: network
network: Classes for Relational Data
Version 1.13.0 created on 2015-08-31.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Martina Morris, University of Washington
                    Skye Bender-deMoll, University of Washington
 For citation information, type citation("network").
 Type help("network-package") to get started.


Attaching package: ‘network’

The following objects are masked from ‘package:igraph’:

    add.edges, add.vertices, %c%, delete.edges, delete.vertices,
    get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite,
    is.directed, list.edge.attributes, list.vertex.attributes, %s%,
    set.edge.attribute, set.vertex.attribute


ergm: version 3.7.1, created on 2017-03-20
Copyright (c) 2017, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Martina Morris, University of Washington
                    with contributions from
                    Li Wang
                    Kirk Li, University of Washington
                    Skye Bender-deMoll, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm").

NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()
constriant which distorted the sampled distribution somewhat. In
addition, Sampson's Monks datasets had mislabeled verteces. See the
NEWS and the documentation for more details.

> library(parallel)
> library(doParallel)
Loading required package: foreach
Loading required package: iterators
> library(lubridate)

Attaching package: ‘lubridate’

The following object is masked from ‘package:igraph’:

    %--%

The following object is masked from ‘package:base’:

    date

> #/* library(Rmpi) */
> library(snow)

Attaching package: ‘snow’

The following objects are masked from ‘package:parallel’:

    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,
    clusterExport, clusterMap, clusterSplit, makeCluster, parApply,
    parCapply, parLapply, parRapply, parSapply, splitIndices,
    stopCluster

> library(methods)
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:lubridate’:

    intersect, setdiff, union

The following objects are masked from ‘package:igraph’:

    %>%, as_data_frame, groups, union

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> library(stringr)

Attaching package: ‘stringr’

The following object is masked from ‘package:igraph’:

    %>%

> 
> #' Loading the variables from the last section...
> 
> #+ var-load,cache=FALSE
> load('./Rdata/mnet.rda')
> load('./Rdata/mnet2.rda')
> load('./Rdata/mnet2.gc.rda')
> load('./Rdata/auth_data.rda')
> load('./Rdata/edges.rda')
> load('./Rdata/mnet2.gc.rda')
> 
> # /* Loading a simplified version of the network */
> mnet3 <- read_graph('./graphs/CAnet_graph.graphml', format = 'graphml')
> mnet.w <- read_graph('./graphs/CAnet_weight.graphml', format = 'graphml')
> 
> #' Some data manipulations: Defining Local, Regional and International actors
> #+ manip, cahce=FALSE
> newCountry<-V(mnet2)$country %>% str_replace("\n", "")
> V(mnet2)$country<-newCountry
> cont<-read.csv("continent.csv", header = T)
> continent<-list()
> ctnt<-as.vector(cont$Continent)
> ctry<-as.vector(cont$Country)
> for(i in 1:length(ctnt)){
+   continent[[toupper(ctry[i])]]<-toupper(ctnt[i])
+ }
> # V(mnet2)$continent<-numeric(length(V(mnet)$country))
> newContinent<-c()
> for(i in 1:length(V(mnet2)$country)){
+   entry<-continent[[V(mnet2)$country[i]]]
+   if(is.null(entry)){entry<-''}
+   newContinent<-c(newContinent, entry)
+ }
> V(mnet2)$continent<-newContinent
> 
> # Assigning collaboration scope
> collabType<-c()
> for(i in 1:length(V(mnet2)$continent)){
+   collabScope=''
+   if(V(mnet2)$country[i]=='BENIN' && V(mnet2)$continent[i]=='AFRICA'){
+     collabScope<-'NATIONAL'
+   } else if(V(mnet2)$country[i]!='BENIN' && V(mnet2)$continent[i]=='AFRICA'){
+     collabScope<-'REGIONAL'
+   } else if(V(mnet2)$continent[i]!='AFRICA' && V(mnet2)$continent[i]!='') {
+     collabScope<-'INTERNATIONAL'
+   } else{
+     collabScope<-NA
+   }
+   collabType<-c(collabType,collabScope)
+ }
> V(mnet2)$collabType<-collabType
> 
> 
> #' Since this processing is going to take a lot of time, we decide to parallelize all the computations:
> #+ detect-cores
> # /* create a cluster with fixed number of processes */
> cores=detectCores() # get number of cores
> # cl <- makeCluster(cores[1]-2) #I decide to run leave 2 cores out
> # registerDoParallel(cl)
> #'
> #+ preproc,cache=FALSE
> A <- get.adjacency(mnet3)
> v.attrs <- get.data.frame(mnet3, what="vertices")
> v.attrs$name <- V(mnet.w)$name
> v.attrs$timesCited <- V(mnet.w)$timesCited
> v.attrs$numPub <- V(mnet.w)$numPub
> v.attrs$community <- V(mnet2)$community
> v.attrs$degree <- V(mnet2)$degree
> v.attrs$affiliation <- V(mnet.w)$place
> v.attrs$city <- V(mnet.w)$affil
> v.attrs$country <- V(mnet.w)$country
> v.attrs$collabType <- V(mnet2)$collabType
> 
> 
> mnet3.simple <- network::as.network(as.matrix(A), directed=FALSE)
> network::set.vertex.attribute(mnet3.simple, "timesCited", V(mnet.w)$timesCited)
> network::set.vertex.attribute(mnet3.simple, "numPub", V(mnet.w)$numPub)
> network::set.vertex.attribute(mnet3.simple, "community", V(mnet2)$community)
> network::set.vertex.attribute(mnet3.simple, "degree", V(mnet2)$degree)
> network::set.vertex.attribute(mnet3.simple, "collabType", V(mnet2)$collabType)
> 
> # pairCited <- get.adjacency(mnet2,attr = 'timesCited')
> # nCollab <- as_adjacency_matrix(mnet2,attr = 'weight')
> #
> # # Adding edge attributes
> # mnet3.simple %e% 'pairCited' <- A
> # mnet3.simple %e% 'nCollab' <- nCollab
> mnet3.simple
 Network attributes:
  vertices = 1792 
  directed = FALSE 
  hyper = FALSE 
  loops = FALSE 
  multiple = FALSE 
  bipartite = FALSE 
  total edges= 95707 
    missing edges= 0 
    non-missing edges= 95707 

 Vertex attribute names: 
    collabType community degree numPub timesCited vertex.names 

 Edge attribute names not shown 
> #'
> #+ formula1
> mod1.f <- formula(mnet3.simple ~ edges)
> mod1.f
mnet3.simple ~ edges
> mnet3.simple ~ edges
mnet3.simple ~ edges
> summary.statistics(mod1.f)
edges 
95707 
> #'
> #+ model-fitting1,cache=FALSE
> t0 <- Sys.time()
> mod1 <- ergm(mod1.f, iterations=1000,
+                          control=control.ergm(parallel=cores, parallel.type="PSOCK", MCMC.samplesize=1000,
+                                               MCMC.interval=500, MCMLE.maxit = 1000, MCMC.burnin = 20000, seed=25))
Evaluating log-likelihood at the estimate. 
> Sys.time() - t0
Time difference of 3.926986 secs
> # save(my.ergm.bern.fit, file = './Rdata/my.ergm.bern.fit.rda')
> #'
> #+ model-summary,cache=FALSE
> anova.ergm(mod1)
Analysis of Variance Table

Model 1: mnet3.simple ~ edges
         Df Deviance Resid. Df Resid. Dev Pr(>|Chisq|)    
NULL                   1604736          0                 
Model 1:  1  -725266   1604735     725266    < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> summary(mod1)

==========================
Summary of model fit
==========================

Formula:   mnet3.simple ~ edges

Iterations:  6 out of 1000 

Monte Carlo MLE Results:
       Estimate Std. Error MCMC % p-value    
edges -2.757930   0.003333      0  <1e-04 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Null Deviance: 2224636  on 1604736  degrees of freedom
 Residual Deviance:  725266  on 1604735  degrees of freedom
 
AIC: 725268    BIC: 725280    (Smaller is better.) 
> #'
> #+ save-mod1,cache=FALSE
> save(mod1, file = 'modData2/mod1.rda')
> 
> 
> #' The coefficient of this model is {{mod1$coef}}
> #'  and the model likelihood is {{mod1$mle.lik}}
> #' The coefficient estimate tell us that if this network had been generated by
> #' the posited model, then the probability corresponding to the log-odds of a tie should is {{plogis(mod1$coef)}}
> #' . This probability also represents the
> #' fraction of possible edges that are realized. The amount of deviance explained by the model
> #' is represented by two standard measures of model fit based on the likelihood (AIC and BIC),
> #' and the MCMC standard error. Since this model exhibits dyadic independence,
> #' MCMC estimation is not necessary, and the MCMC standard error does not apply.
> 
> #' From our section on the mathematical modeling of our network, we know that our network is not random.
> #' To explore alternative hypotheses about the structure that exists and the underlying processes
> #' that generated our network, we rely on the strength of ERGM to model building.
> #'
> #' From previous analyses, we have shown that prolific authors in our network tend to collaborate more.
> #' We have also demonstrated a tendency towards collaboration within authors of the same clusters/communities.
> #'
> #' Therefore, a good model to begin with will be a model that proposes assortative mixing i.e a greater
> #' probability of authors collaborating with other authors of the same productivity
> #' (number of publications, number of times cited, or research cluster/community). Furthermore, this model
> #' is convenient in that it exhibits dyadic independence. [Hunter et al. (2008b)].
> #' We assume a uniform tendency towards assortative mixing within each attribute class
> #' (same tendency to collaborate for within cluster/community)
> #' We fit the model using the nodecov term (main effect of a continuous covariate), and the nodematch term
> #' (uniform homophily) for our categorical variable "community".
> 
> #+ formula2,cache=FALSE
> mod2.f <- formula(mnet3.simple ~ edges + nodecov("timesCited") + nodecov('degree') +
+                         nodecov("numPub") + nodematch("community") + nodematch("collabType"))
> summary.statistics(mod2.f)
               edges   nodecov.timesCited       nodecov.degree 
               95707             33146116             56641204 
      nodecov.numPub  nodematch.community nodematch.collabType 
              495605                92011                58334 
> #'
> #+ model-fitting2,cache=FALSE
> t0 <- Sys.time()
> mod2 <- ergm(mod2.f, iterations=1000,
+                        control=control.ergm(parallel=cores, parallel.type="PSOCK", MCMC.samplesize=1000,
+                                             MCMC.interval=5000, MCMC.burnin = 20000, seed=25, MCMLE.maxit = 1000))
Evaluating log-likelihood at the estimate. 
> # save(mod1, file = './Rdata/mod1.rda')
> Sys.time() - t0
Time difference of 11.51102 secs
> summary(mod2)

==========================
Summary of model fit
==========================

Formula:   mnet3.simple ~ edges + nodecov("timesCited") + nodecov("degree") + 
    nodecov("numPub") + nodematch("community") + nodematch("collabType")

Iterations:  8 out of 1000 

Monte Carlo MLE Results:
                       Estimate Std. Error MCMC % p-value    
edges                -1.144e+01  3.429e-02      0  <1e-04 ***
nodecov.timesCited   -1.306e-03  5.489e-05      0  <1e-04 ***
nodecov.degree        1.371e-02  5.168e-05      0  <1e-04 ***
nodecov.numPub        2.320e-02  8.955e-04      0  <1e-04 ***
nodematch.community   7.415e+00  2.924e-02      0  <1e-04 ***
nodematch.collabType  4.514e-01  1.598e-02      0  <1e-04 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Null Deviance: 2224636  on 1604736  degrees of freedom
 Residual Deviance:  137925  on 1604730  degrees of freedom
 
AIC: 137937    BIC: 138011    (Smaller is better.) 
> anova.ergm(mod2)
Analysis of Variance Table

Model 1: mnet3.simple ~ edges + nodecov("timesCited") + nodecov("degree") + 
    nodecov("numPub") + nodematch("community") + nodematch("collabType")
         Df Deviance Resid. Df Resid. Dev Pr(>|Chisq|)    
NULL                   1604736          0                 
Model 1:  6  -137925   1604730     137925    < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> #'
> #+ save-mod2,cache=FALSE
> save(mod2, file = 'modData2/mod2.rda')
> 
> 
> #' The new model converges after {{mod2$iterations}}
> #'  iterations. We can see that all 4 terms are significant with a dramatic
> #' improvement in model likelihood. Model likelihood = {{mod2$mle.lik}}
> #'
> #' The log-odds of a collaboration that is completely heterogeneous is {{round(mod2$coef[1],2)}}
> #' , hence the completely heterogeneous probability of a two authors collaborating is {{plogis(mod2$coef[1])}}
> #'
> #' . The log-odds of a collaboration homogeneous between by communities is {{round(mod2$coef[4],2)}}
> #' , the homogeneous probability of two authors collaborating between the same community is {{plogis(mod2$coef[5])}}
> #' , and the homogeneous probability of two authors with the same collaboration type collaborating is {{plogis(mod2$coef[6])}}
> #' .
> #' The log-odds of a collaboration that is homogeneous in all 5 attributes is {{sum(mod2$coef)}}
> #'  and the associated probability is {{plogis(sum(mod2$coef))}}
> #'
> #' All the probabilities for each attribute are: {{plogis(mod2$coef)}}
> #'
> #' Often times, it is typical to include the nodefactor term to capture interaction or second order effects
> #' of attributes.
> #'
> #' Let's run another model with this new term:
> 
> #+ formula3,cache=FALSE
> mod3.f <- formula(mnet3.simple ~ edges + nodecov("timesCited") + nodecov('degree') + nodecov("numPub") + nodematch("community") + nodematch("collabType") + nodefactor("collabType"))
> summary.statistics(mod3.f)
                         edges             nodecov.timesCited 
                         95707                       33146116 
                nodecov.degree                 nodecov.numPub 
                      56641204                         495605 
           nodematch.community           nodematch.collabType 
                         92011                          58334 
nodefactor.collabType.NATIONAL nodefactor.collabType.REGIONAL 
                          6124                          30809 
> #'
> #+ model-fitting3,cache=FALSE
> t0 <- Sys.time()
> mod3 <- ergm(mod3.f, iterations=1000,
+                        control=control.ergm(parallel=cores, parallel.type="PSOCK", MCMC.samplesize=1000,
+                                             MCMC.interval=5000, MCMC.burnin = 20000, seed=25, MCMLE.maxit = 1000))
Evaluating log-likelihood at the estimate. 
> #save(mod2, file = './Rdata/mod2.rda')
> Sys.time() - t0
Time difference of 14.7932 secs
> summary(mod3)

==========================
Summary of model fit
==========================

Formula:   mnet3.simple ~ edges + nodecov("timesCited") + nodecov("degree") + 
    nodecov("numPub") + nodematch("community") + nodematch("collabType") + 
    nodefactor("collabType")

Iterations:  9 out of 1000 

Monte Carlo MLE Results:
                                 Estimate Std. Error MCMC % p-value    
edges                          -1.157e+01  3.617e-02      0  <1e-04 ***
nodecov.timesCited             -1.300e-03  5.403e-05      0  <1e-04 ***
nodecov.degree                  1.357e-02  5.371e-05      0  <1e-04 ***
nodecov.numPub                  2.494e-02  8.936e-04      0  <1e-04 ***
nodematch.community             7.342e+00  2.896e-02      0  <1e-04 ***
nodematch.collabType            5.304e-01  1.634e-02      0  <1e-04 ***
nodefactor.collabType.NATIONAL  2.245e-02  1.705e-02      0   0.188    
nodefactor.collabType.REGIONAL  3.401e-01  1.274e-02      0  <1e-04 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

     Null Deviance: 2224636  on 1604736  degrees of freedom
 Residual Deviance:  137218  on 1604728  degrees of freedom
 
AIC: 137234    BIC: 137332    (Smaller is better.) 
> anova.ergm(mod3)
Analysis of Variance Table

Model 1: mnet3.simple ~ edges + nodecov("timesCited") + nodecov("degree") + 
    nodecov("numPub") + nodematch("community") + nodematch("collabType") + 
    nodefactor("collabType")
         Df Deviance Resid. Df Resid. Dev Pr(>|Chisq|)    
NULL                   1604736          0                 
Model 1:  8  -137218   1604728     137218    < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> #'
> #+ save-mod3,cache=FALSE
> save(mod3, file = 'modData2/mod3.rda')
> 
> 
> #' The new model converges after {{mod3$iterations}}
> #'  iterations. We can see that all terms are significant a
> #' model likelihood of {{mod3$mle.lik}}
> #' .
> #' Let's see what this model does and does not capture about the structure in our original data.
> #' To do this, we use the model fit that we have just generated to simulate new networks at random,
> #' and consider how these are similar to or different from our data
> 
> #+ simulation1,cache=FALSE
> sim1 <- simulate(mod3,
+                  # burnin = 1e+6, # we may set the number of steps in the simulation chain to 1e+06
+                  verbose = TRUE, seed = 25)
Starting MCMC iterations to generate 1 network
Sampler accepted  12.110% of 20000 proposed steps.
Sample size = 1 by 1 
Finished simulation 1 of 1.
> #'
> #+ save-simulation1,cache=FALSE
> save(sim1, file = 'modData2/sim1.rda')
> 
> #' Let's examine the mixing matrix for the attribute collabType
> 
> #+ mixing-matrix1,cache=FALSE
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> M1 <- mixingmatrix(sim1, "collabType")
> confusionMatrix(M1$matrix)
Confusion Matrix and Statistics

               To
From            INTERNATIONAL NATIONAL REGIONAL
  INTERNATIONAL         53139     2374    19959
  NATIONAL               2374     1180      754
  REGIONAL              19959      754     4049

Overall Statistics
                                          
               Accuracy : 0.5583          
                 95% CI : (0.5553, 0.5613)
    No Information Rate : 0.7219          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : -0.0491         
 Mcnemar's Test P-Value : 1               

Statistics by Class:

                     Class: INTERNATIONAL Class: NATIONAL Class: REGIONAL
Sensitivity                        0.7041         0.27391         0.16352
Specificity                        0.2318         0.96879         0.74037
Pos Pred Value                     0.7041         0.27391         0.16352
Neg Pred Value                     0.2318         0.96879         0.74037
Prevalence                         0.7219         0.04121         0.23686
Detection Rate                     0.5083         0.01129         0.03873
Detection Prevalence               0.7219         0.04121         0.23686
Balanced Accuracy                  0.4679         0.62135         0.45195
> 
> #' Let's examine the mixing matrix for the attribute community
> #' #+ mixing-matrix1b,cache=FALSE
> M2 <- mixingmatrix(sim1, "community")
> confusionMatrix(M2$matrix)
Confusion Matrix and Statistics

    To
From     1     2     3     4     5     6     7     8     9    10    11    12
  1  65703    59    49     2     0     0     1     0     0     1     0     0
  2     59  4909   399     0     0     0     0     0     0     0     0     0
  3     49   399  3988    33     0     0     0     0     0     0     0     0
  4      2     0    33   445     0     0     0     0     0     0     0     0
  5      0     0     0     0   117     0     0     0     0     0     0     0
  6      0     0     0     0     0    43     0     0     0     0     0     0
  7      1     0     0     0     0     0    31     0     0     0     0     0
  8      0     0     0     0     0     0     0    25     0     0     0     0
  9      0     0     0     0     0     0     0     0    13     0     0     0
  10     1     0     0     0     0     0     0     0     0    19     0     0
  11     0     0     0     0     0     0     0     0     0     0     9     0
  12     0     0     0     0     0     0     0     0     0     0     0     9
  13     0     0     0     0     0     0     0     0     0     0     0     0
  14     0     0     0     0     0     0     0     0     0     0     0     0
  15     0     0     0     0     0     0     0     0     0     0     0     0
  16     1     0     0     0     0     0     0     0     0     0     0     0
  17     0     0     0     0     0     0     0     0     0     0     0     0
  18     0     0     0     0     0     0     0     0     0     0     0     0
  19     0     0     0     0     0     0     0     0     0     0     0     0
  20     0     0     0     0     0     0     0     0     0     0     0     0
  21     2     1     0     0     0     0     0     0     0     0     0     0
  22   859   503  1656     1     0     0     0     0     0     0     0     0
  23     3    11    26     0     0     0     0     0     0     0     0     0
    To
From    13    14    15    16    17    18    19    20    21    22    23
  1      0     0     0     1     0     0     0     0     2   859     3
  2      0     0     0     0     0     0     0     0     1   503    11
  3      0     0     0     0     0     0     0     0     0  1656    26
  4      0     0     0     0     0     0     0     0     0     1     0
  5      0     0     0     0     0     0     0     0     0     0     0
  6      0     0     0     0     0     0     0     0     0     0     0
  7      0     0     0     0     0     0     0     0     0     0     0
  8      0     0     0     0     0     0     0     0     0     0     0
  9      0     0     0     0     0     0     0     0     0     0     0
  10     0     0     0     0     0     0     0     0     0     0     0
  11     0     0     0     0     0     0     0     0     0     0     0
  12     0     0     0     0     0     0     0     0     0     0     0
  13     9     0     0     0     0     0     0     0     0     0     0
  14     0    10     0     0     0     0     0     0     0     0     0
  15     0     0     9     0     0     0     0     0     0     0     0
  16     0     0     0     3     0     0     0     0     0     0     0
  17     0     0     0     0     2     0     0     0     0     0     0
  18     0     0     0     0     0     2     0     0     0     0     0
  19     0     0     0     0     0     0     1     0     0     0     0
  20     0     0     0     0     0     0     0     1     0     0     0
  21     0     0     0     0     0     0     0     0   164    70     0
  22     0     0     0     0     0     0     0     0    70 16279     1
  23     0     0     0     0     0     0     0     0     0     1   308

Overall Statistics
                                          
               Accuracy : 0.926           
                 95% CI : (0.9244, 0.9277)
    No Information Rate : 0.6705          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.8536          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: 1 Class: 2 Class: 3 Class: 4 Class: 5  Class: 6
Sensitivity            0.9853  0.83458  0.64835 0.925156 1.000000 1.0000000
Specificity            0.9702  0.98960  0.97682 0.999636 1.000000 1.0000000
Pos Pred Value         0.9853  0.83458  0.64835 0.925156 1.000000 1.0000000
Neg Pred Value         0.9702  0.98960  0.97682 0.999636 1.000000 1.0000000
Prevalence             0.6705  0.05914  0.06185 0.004836 0.001176 0.0004324
Detection Rate         0.6606  0.04936  0.04010 0.004474 0.001176 0.0004324
Detection Prevalence   0.6705  0.05914  0.06185 0.004836 0.001176 0.0004324
Balanced Accuracy      0.9778  0.91209  0.81258 0.962396 1.000000 1.0000000
                      Class: 7  Class: 8  Class: 9 Class: 10 Class: 11
Sensitivity          0.9687500 1.0000000 1.0000000 0.9500000 1.000e+00
Specificity          0.9999899 1.0000000 1.0000000 0.9999899 1.000e+00
Pos Pred Value       0.9687500 1.0000000 1.0000000 0.9500000 1.000e+00
Neg Pred Value       0.9999899 1.0000000 1.0000000 0.9999899 1.000e+00
Prevalence           0.0003218 0.0002514 0.0001307 0.0002011 9.049e-05
Detection Rate       0.0003117 0.0002514 0.0001307 0.0001910 9.049e-05
Detection Prevalence 0.0003218 0.0002514 0.0001307 0.0002011 9.049e-05
Balanced Accuracy    0.9843700 1.0000000 1.0000000 0.9749950 1.000e+00
                     Class: 12 Class: 13 Class: 14 Class: 15 Class: 16
Sensitivity          1.000e+00 1.000e+00 1.0000000 1.000e+00 7.500e-01
Specificity          1.000e+00 1.000e+00 1.0000000 1.000e+00 1.000e+00
Pos Pred Value       1.000e+00 1.000e+00 1.0000000 1.000e+00 7.500e-01
Neg Pred Value       1.000e+00 1.000e+00 1.0000000 1.000e+00 1.000e+00
Prevalence           9.049e-05 9.049e-05 0.0001005 9.049e-05 4.022e-05
Detection Rate       9.049e-05 9.049e-05 0.0001005 9.049e-05 3.016e-05
Detection Prevalence 9.049e-05 9.049e-05 0.0001005 9.049e-05 4.022e-05
Balanced Accuracy    1.000e+00 1.000e+00 1.0000000 1.000e+00 8.750e-01
                     Class: 17 Class: 18 Class: 19 Class: 20 Class: 21
Sensitivity          1.000e+00 1.000e+00 1.000e+00 1.000e+00  0.691983
Specificity          1.000e+00 1.000e+00 1.000e+00 1.000e+00  0.999264
Pos Pred Value       1.000e+00 1.000e+00 1.000e+00 1.000e+00  0.691983
Neg Pred Value       1.000e+00 1.000e+00 1.000e+00 1.000e+00  0.999264
Prevalence           2.011e-05 2.011e-05 1.005e-05 1.005e-05  0.002383
Detection Rate       2.011e-05 2.011e-05 1.005e-05 1.005e-05  0.001649
Detection Prevalence 2.011e-05 2.011e-05 1.005e-05 1.005e-05  0.002383
Balanced Accuracy    1.000e+00 1.000e+00 1.000e+00 1.000e+00  0.845624
                     Class: 22 Class: 23
Sensitivity             0.8405  0.882521
Specificity             0.9614  0.999586
Pos Pred Value          0.8405  0.882521
Neg Pred Value          0.9614  0.999586
Prevalence              0.1948  0.003509
Detection Rate          0.1637  0.003097
Detection Prevalence    0.1948  0.003509
Balanced Accuracy       0.9009  0.941054
> 
> #' This model captures more the research community with an accuracy of 92.57%. It performed less
> #' with the collaboration type with an accuracy of 55,82%
> 
> #' Identifying Model degeneracy
> #' We first fit a triangle model
> 
> #+ degeneracy-checking,cache=FALSE
> mod4.f <- formula(mnet3.simple ~ edges + triangle)
> t0 <- Sys.time()
> mod4 <- ergm(mod4.f, maxit=1000, verbose = TRUE,
+              control = control.ergm(MCMLE.maxit=1000, MCMC.samplesize = 10000, MCMC.interval = 1000, seed=25))
Evaluating network in model
Initializing Metropolis-Hastings proposal(s): ergm:MH_TNT
Initializing model.
Using initial method 'MPLE'.
Fitting initial model.
MPLE covariate matrix has 236 rows.
Fitting ERGM.
Starting maximum likelihood estimation via MCMLE:
Density guard set to 1922326 from an initial count of 95707  edges.
Iteration 1 of at most 1000 with parameter: 
      edges    triangle 
-5.10599782  0.06176146 
Sampler accepted  11.051% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1011.962 241613.520 
Average estimating equation values:
     edges   triangle 
  1011.962 241613.520 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.797856, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, inside hull.
iter= 2, est=0.904493, low=0.797856, high=1.000000, test=1.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.853503, low=0.797856, high=0.904493, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.826427, low=0.797856, high=0.853503, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.812354, low=0.797856, high=0.826427, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.819440, low=0.812354, high=0.826427, test=1.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.822946, low=0.819440, high=0.826427, test=1.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.824690, low=0.822946, high=0.826427, test=1.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.825559, low=0.824690, high=0.826427, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.825993, low=0.825559, high=0.826427, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.825993365000031  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 8.67 
Iteration 2 of at most 1000 with parameter: 
      edges    triangle 
-5.07478575  0.06154384 
Sampler accepted  11.258% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2372.102 417531.374 
Average estimating equation values:
     edges   triangle 
  2372.102 417531.374 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.719257, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.553634, low=0.000000, high=0.719257, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.428270, low=0.000000, high=0.553634, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.325739, low=0.000000, high=0.428270, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.239678, low=0.000000, high=0.325739, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.167970, low=0.000000, high=0.239678, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.110555, low=0.000000, high=0.167970, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.067801, low=0.000000, high=0.110555, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.038869, low=0.000000, high=0.067801, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.021117, low=0.000000, high=0.038869, test=0.
is.inCH: iter= 1, inside hull.
iter= 11, est=0.030341, low=0.021117, high=0.038869, test=1.
is.inCH: iter= 1, inside hull.
iter= 12, est=0.034685, low=0.030341, high=0.038869, test=1.
is.inCH: iter= 1, inside hull.
iter= 13, est=0.036796, low=0.034685, high=0.038869, test=1.
is.inCH: iter= 1, inside hull.
iter= 14, est=0.037838, low=0.036796, high=0.038869, test=1.
is.inCH: iter= 1, inside hull.
iter= 15, est=0.038355, low=0.037838, high=0.038869, test=1.
is.inCH: iter= 1, outside hull.
iter= 16, est=0.038096, low=0.037838, high=0.038355, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0380964361011077  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.101 
Iteration 3 of at most 1000 with parameter: 
      edges    triangle 
-5.07088711  0.06125747 
Sampler accepted  11.299% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2492.003 428280.255 
Average estimating equation values:
     edges   triangle 
  2492.003 428280.255 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.216887, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.103999, low=0.000000, high=0.216887, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.051792, low=0.000000, high=0.103999, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.025942, low=0.000000, high=0.051792, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012994, low=0.000000, high=0.025942, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.006505, low=0.000000, high=0.012994, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.003254, low=0.000000, high=0.006505, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001628, low=0.000000, high=0.003254, test=0.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.002441, low=0.001628, high=0.003254, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.002848, low=0.002441, high=0.003254, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.00284775271390053  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 4.855 
Iteration 4 of at most 1000 with parameter: 
      edges    triangle 
-5.05451535  0.05320073 
Sampler accepted  11.319% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2485.183 420980.989 
Average estimating equation values:
     edges   triangle 
  2485.183 420980.989 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.203202, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.096108, low=0.000000, high=0.203202, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047478, low=0.000000, high=0.096108, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023674, low=0.000000, high=0.047478, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011830, low=0.000000, high=0.023674, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005914, low=0.000000, high=0.011830, test=0.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.008872, low=0.005914, high=0.011830, test=1.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.007393, low=0.005914, high=0.008872, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.006654, low=0.005914, high=0.007393, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.006284, low=0.005914, high=0.006654, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.00628394407194543  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 0.7105 
Iteration 5 of at most 1000 with parameter: 
      edges    triangle 
-5.05374281  0.05265901 
Sampler accepted  11.324% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2466.832 416054.877 
Average estimating equation values:
     edges   triangle 
  2466.832 416054.877 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.204488, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.096840, low=0.000000, high=0.204488, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047875, low=0.000000, high=0.096840, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023881, low=0.000000, high=0.047875, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011936, low=0.000000, high=0.023881, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005968, low=0.000000, high=0.011936, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002984, low=0.000000, high=0.005968, test=0.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.004476, low=0.002984, high=0.005968, test=1.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.005222, low=0.004476, high=0.005968, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.005595, low=0.005222, high=0.005968, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.00559513541368111  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.79 
Iteration 6 of at most 1000 with parameter: 
     edges   triangle 
-5.0489149  0.0502337 
Sampler accepted  11.336% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2415.047 409059.961 
Average estimating equation values:
     edges   triangle 
  2415.047 409059.961 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.204229, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.096692, low=0.000000, high=0.204229, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047795, low=0.000000, high=0.096692, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023839, low=0.000000, high=0.047795, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011915, low=0.000000, high=0.023839, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.017875, low=0.011915, high=0.023839, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.014894, low=0.011915, high=0.017875, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.013404, low=0.011915, high=0.014894, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.012659, low=0.011915, high=0.013404, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.012287, low=0.011915, high=0.012659, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0122870657069654  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.024 
Iteration 7 of at most 1000 with parameter: 
     edges   triangle 
-5.0496433  0.0498307 
Sampler accepted  11.321% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2334.987 400978.435 
Average estimating equation values:
     edges   triangle 
  2334.987 400978.435 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.206758, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.098137, low=0.000000, high=0.206758, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.048580, low=0.000000, high=0.098137, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024251, low=0.000000, high=0.048580, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012125, low=0.000000, high=0.024251, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.006064, low=0.000000, high=0.012125, test=0.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.009095, low=0.006064, high=0.012125, test=1.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.010610, low=0.009095, high=0.012125, test=1.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.009853, low=0.009095, high=0.010610, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.010231, low=0.009853, high=0.010610, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0102313647782741  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.513 
Iteration 8 of at most 1000 with parameter: 
      edges    triangle 
-5.04829431  0.04908539 
Sampler accepted  11.321% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
    edges  triangle 
  2345.62 397317.37 
Average estimating equation values:
    edges  triangle 
  2345.62 397317.37 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.205977, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.097690, low=0.000000, high=0.205977, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.048337, low=0.000000, high=0.097690, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024123, low=0.000000, high=0.048337, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012060, low=0.000000, high=0.024123, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.006031, low=0.000000, high=0.012060, test=0.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.009046, low=0.006031, high=0.012060, test=1.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.007538, low=0.006031, high=0.009046, test=0.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.008292, low=0.007538, high=0.009046, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.008669, low=0.008292, high=0.009046, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.00866873295058324  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.604 
Iteration 9 of at most 1000 with parameter: 
      edges    triangle 
-5.03996842  0.04752419 
Sampler accepted  11.395% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2350.393 386282.112 
Average estimating equation values:
     edges   triangle 
  2350.393 386282.112 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.205386, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.097352, low=0.000000, high=0.205386, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.048153, low=0.000000, high=0.097352, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024027, low=0.000000, high=0.048153, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012011, low=0.000000, high=0.024027, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.018017, low=0.012011, high=0.024027, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.015014, low=0.012011, high=0.018017, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.013512, low=0.012011, high=0.015014, test=0.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.014263, low=0.013512, high=0.015014, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.014638, low=0.014263, high=0.015014, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.014638176119418  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.136 
Iteration 10 of at most 1000 with parameter: 
      edges    triangle 
-5.04220581  0.04713606 
Sampler accepted  11.346% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2259.407 375905.497 
Average estimating equation values:
     edges   triangle 
  2259.407 375905.497 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.207656, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.098652, low=0.000000, high=0.207656, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.048860, low=0.000000, high=0.098652, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024398, low=0.000000, high=0.048860, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012201, low=0.000000, high=0.024398, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.018299, low=0.012201, high=0.024398, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.015250, low=0.012201, high=0.018299, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.013725, low=0.012201, high=0.015250, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.012963, low=0.012201, high=0.013725, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.012582, low=0.012201, high=0.012963, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0125819658359493  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.774 
Iteration 11 of at most 1000 with parameter: 
      edges    triangle 
-5.04047110  0.04637528 
Sampler accepted  11.371% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2219.985 368999.350 
Average estimating equation values:
     edges   triangle 
  2219.985 368999.350 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.206871, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.098201, low=0.000000, high=0.206871, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.048615, low=0.000000, high=0.098201, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024269, low=0.000000, high=0.048615, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012135, low=0.000000, high=0.024269, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.018201, low=0.012135, high=0.024269, test=1.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.021234, low=0.018201, high=0.024269, test=1.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.022752, low=0.021234, high=0.024269, test=1.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.023510, low=0.022752, high=0.024269, test=1.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.023131, low=0.022752, high=0.023510, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0231309182362714  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.563 
Iteration 12 of at most 1000 with parameter: 
      edges    triangle 
-5.04084853  0.04601141 
Sampler accepted  11.372% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2196.304 362025.269 
Average estimating equation values:
     edges   triangle 
  2196.304 362025.269 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.210941, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.100543, low=0.000000, high=0.210941, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.049893, low=0.000000, high=0.100543, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024940, low=0.000000, high=0.049893, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.012479, low=0.000000, high=0.024940, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.018711, low=0.012479, high=0.024940, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.015595, low=0.012479, high=0.018711, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.014038, low=0.012479, high=0.015595, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.013258, low=0.012479, high=0.014038, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.013648, low=0.013258, high=0.014038, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0136480001701444  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 3.364 
Iteration 13 of at most 1000 with parameter: 
     edges   triangle 
-5.0384613  0.0446354 
Sampler accepted  11.346% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  2015.037 338996.826 
Average estimating equation values:
     edges   triangle 
  2015.037 338996.826 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.207277, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.098435, low=0.000000, high=0.207277, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.048742, low=0.000000, high=0.098435, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024336, low=0.000000, high=0.048742, test=0.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.036520, low=0.024336, high=0.048742, test=1.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.042625, low=0.036520, high=0.048742, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.039571, low=0.036520, high=0.042625, test=0.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.041098, low=0.039571, high=0.042625, test=1.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.040334, low=0.039571, high=0.041098, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.040716, low=0.040334, high=0.041098, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0407157998211899  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.033 
Iteration 14 of at most 1000 with parameter: 
      edges    triangle 
-5.04096835  0.04450056 
Sampler accepted  11.338% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1882.719 317147.784 
Average estimating equation values:
     edges   triangle 
  1882.719 317147.784 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.217948, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.104621, low=0.000000, high=0.217948, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.052135, low=0.000000, high=0.104621, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.026123, low=0.000000, high=0.052135, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.013088, low=0.000000, high=0.026123, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.019610, low=0.013088, high=0.026123, test=1.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.022868, low=0.019610, high=0.026123, test=1.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.021239, low=0.019610, high=0.022868, test=0.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.022054, low=0.021239, high=0.022868, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.022461, low=0.022054, high=0.022868, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0224605991696187  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.347 
Iteration 15 of at most 1000 with parameter: 
      edges    triangle 
-5.03824008  0.04410615 
Sampler accepted  11.402% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1860.588 305406.383 
Average estimating equation values:
     edges   triangle 
  1860.588 305406.383 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.210679, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.100392, low=0.000000, high=0.210679, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.049810, low=0.000000, high=0.100392, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.024897, low=0.000000, high=0.049810, test=0.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.037341, low=0.024897, high=0.049810, test=1.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.031117, low=0.024897, high=0.037341, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.028007, low=0.024897, high=0.031117, test=0.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.029562, low=0.028007, high=0.031117, test=1.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.030339, low=0.029562, high=0.031117, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.030728, low=0.030339, high=0.031117, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0307280696033953  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.264 
Iteration 16 of at most 1000 with parameter: 
      edges    triangle 
-5.03724881  0.04383083 
Sampler accepted  11.407% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1747.304 284679.067 
Average estimating equation values:
     edges   triangle 
  1747.304 284679.067 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.213934, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.102277, low=0.000000, high=0.213934, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.050844, low=0.000000, high=0.102277, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.025441, low=0.000000, high=0.050844, test=0.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.038136, low=0.025441, high=0.050844, test=1.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.044487, low=0.038136, high=0.050844, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.041311, low=0.038136, high=0.044487, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.039723, low=0.038136, high=0.041311, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.038930, low=0.038136, high=0.039723, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.039327, low=0.038930, high=0.039723, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0393265650767908  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.078 
Iteration 17 of at most 1000 with parameter: 
      edges    triangle 
-5.03646442  0.04363348 
Sampler accepted  11.422% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1804.993 287482.190 
Average estimating equation values:
     edges   triangle 
  1804.993 287482.190 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.217384, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.104290, low=0.000000, high=0.217384, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.051953, low=0.000000, high=0.104290, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.026026, low=0.000000, high=0.051953, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.013038, low=0.000000, high=0.026026, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.019537, low=0.013038, high=0.026026, test=1.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.022783, low=0.019537, high=0.026026, test=1.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.021160, low=0.019537, high=0.022783, test=0.
is.inCH: iter= 1, inside hull.
iter= 9, est=0.021971, low=0.021160, high=0.022783, test=1.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.022377, low=0.021971, high=0.022783, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0223770275633393  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.522 
Iteration 18 of at most 1000 with parameter: 
      edges    triangle 
-5.04268299  0.04288842 
Sampler accepted  11.381% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1508.587 254632.430 
Average estimating equation values:
     edges   triangle 
  1508.587 254632.430 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.210646, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.100373, low=0.000000, high=0.210646, test=0.
is.inCH: iter= 1, inside hull.
iter= 3, est=0.153283, low=0.100373, high=0.210646, test=1.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.126423, low=0.100373, high=0.153283, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.113312, low=0.100373, high=0.126423, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.106823, low=0.100373, high=0.113312, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.103593, low=0.100373, high=0.106823, test=0.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.105207, low=0.103593, high=0.106823, test=1.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.104400, low=0.103593, high=0.105207, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.103996, low=0.103593, high=0.104400, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.103996423706686  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.324 
Iteration 19 of at most 1000 with parameter: 
      edges    triangle 
-5.04609240  0.04280864 
Sampler accepted  11.397% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  1228.152 200295.470 
Average estimating equation values:
     edges   triangle 
  1228.152 200295.470 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.245572, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.121277, low=0.000000, high=0.245572, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.061515, low=0.000000, high=0.121277, test=0.
is.inCH: iter= 1, inside hull.
iter= 4, est=0.091458, low=0.061515, high=0.121277, test=1.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.106364, low=0.091458, high=0.121277, test=1.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.113817, low=0.106364, high=0.121277, test=1.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.110090, low=0.106364, high=0.113817, test=0.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.111954, low=0.110090, high=0.113817, test=1.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.111022, low=0.110090, high=0.111954, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.110556, low=0.110090, high=0.111022, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.110556207241986  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.114 
Iteration 20 of at most 1000 with parameter: 
      edges    triangle 
-5.04979137  0.04273069 
Sampler accepted  11.386% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
   913.5853 160705.6564 
Average estimating equation values:
      edges    triangle 
   913.5853 160705.6564 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.248658, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, inside hull.
iter= 2, est=0.406346, low=0.248658, high=1.000000, test=1.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.320180, low=0.248658, high=0.406346, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.283189, low=0.248658, high=0.320180, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.265667, low=0.248658, high=0.283189, test=0.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.274358, low=0.265667, high=0.283189, test=1.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.278755, low=0.274358, high=0.283189, test=1.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.276552, low=0.274358, high=0.278755, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.275454, low=0.274358, high=0.276552, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.276003, low=0.275454, high=0.276552, test=1.
is.inCH: iter= 1, inside hull.
iter= 11, est=0.276278, low=0.276003, high=0.276552, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.276277579706966  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 3.076 
Iteration 21 of at most 1000 with parameter: 
      edges    triangle 
-5.04958378  0.04259093 
Sampler accepted  12.205% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -3814.858 -221540.704 
Average estimating equation values:
      edges    triangle 
  -3814.858 -221540.704 
is.inCH: iter= 1, inside hull.
iter= 1, est=1.000000, low=1.000000, high=1.000000, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  1  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 0.144 
Step length converged once. Increasing MCMC sample size.
Iteration 22 of at most 1000 with parameter: 
      edges    triangle 
-5.04951649  0.04259107 
Sampler accepted  13.972% of 40000000 proposed steps.
Sample size = 40000 by 40000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -19047.22 -1031990.61 
Average estimating equation values:
      edges    triangle 
  -19047.22 -1031990.61 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.797856, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.655364, low=0.000000, high=0.797856, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.540677, low=0.000000, high=0.655364, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.442886, low=0.000000, high=0.540677, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.356981, low=0.000000, high=0.442886, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.280499, low=0.000000, high=0.356981, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.212488, low=0.000000, high=0.280499, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.153168, low=0.000000, high=0.212488, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.103648, low=0.000000, high=0.153168, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.065230, low=0.000000, high=0.103648, test=0.
is.inCH: iter= 1, outside hull.
iter= 11, est=0.038203, low=0.000000, high=0.065230, test=0.
is.inCH: iter= 1, outside hull.
iter= 12, est=0.021074, low=0.000000, high=0.038203, test=0.
is.inCH: iter= 1, outside hull.
iter= 13, est=0.011146, low=0.000000, high=0.021074, test=0.
is.inCH: iter= 1, outside hull.
iter= 14, est=0.005744, low=0.000000, high=0.011146, test=0.
is.inCH: iter= 1, outside hull.
iter= 15, est=0.002918, low=0.000000, high=0.005744, test=0.
is.inCH: iter= 1, outside hull.
iter= 16, est=0.001471, low=0.000000, high=0.002918, test=0.
is.inCH: iter= 1, outside hull.
iter= 17, est=0.000738, low=0.000000, high=0.001471, test=0.
is.inCH: iter= 1, inside hull.
iter= 18, est=0.001105, low=0.000738, high=0.001471, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.00110526986777648  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.108 
Iteration 23 of at most 1000 with parameter: 
      edges    triangle 
-5.05186785  0.04633046 
Sampler accepted  13.997% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -19053.54 -1031694.74 
Average estimating equation values:
      edges    triangle 
  -19053.54 -1031694.74 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202554, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095739, low=0.000000, high=0.202554, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047278, low=0.000000, high=0.095739, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023569, low=0.000000, high=0.047278, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011776, low=0.000000, high=0.023569, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005887, low=0.000000, high=0.011776, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002944, low=0.000000, high=0.005887, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001472, low=0.000000, high=0.002944, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000736, low=0.000000, high=0.001472, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000368, low=0.000000, high=0.000736, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000367943712112517  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 0.5549 
Iteration 24 of at most 1000 with parameter: 
      edges    triangle 
-5.05812287  0.04936932 
Sampler accepted  13.974% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -19067.75 -1030996.19 
Average estimating equation values:
      edges    triangle 
  -19067.75 -1030996.19 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202281, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202281, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000367224810052669  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.274 
Iteration 25 of at most 1000 with parameter: 
      edges    triangle 
-5.06136364  0.05615938 
Sampler accepted  14.033% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -18998.42 -1030594.24 
Average estimating equation values:
      edges    triangle 
  -18998.42 -1030594.24 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000367224110185951  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.391 
Iteration 26 of at most 1000 with parameter: 
      edges    triangle 
-5.07726560  0.06908795 
Sampler accepted  14.054% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -19002.45 -1030179.19 
Average estimating equation values:
      edges    triangle 
  -19002.45 -1030179.19 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000367224109504568  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.125 
Iteration 27 of at most 1000 with parameter: 
      edges    triangle 
-5.09263872  0.08060411 
Sampler accepted  14.086% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -18940.13 -1029692.51 
Average estimating equation values:
      edges    triangle 
  -18940.13 -1029692.51 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0003672241095039  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.985 
Iteration 28 of at most 1000 with parameter: 
      edges    triangle 
-5.11035883  0.09142823 
Sampler accepted  14.070% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -18968.74 -1029257.41 
Average estimating equation values:
      edges    triangle 
  -18968.74 -1029257.41 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0003672241095039  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.646 
Iteration 29 of at most 1000 with parameter: 
    edges  triangle 
-5.127948  0.100463 
Sampler accepted  14.061% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
  -18989.58 -1028745.39 
Average estimating equation values:
      edges    triangle 
  -18989.58 -1028745.39 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0003672241095039  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.301 
Iteration 30 of at most 1000 with parameter: 
     edges   triangle 
-5.1390169  0.1075571 
Sampler accepted  14.021% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
 -18332.39 -960153.43 
Average estimating equation values:
     edges   triangle 
 -18332.39 -960153.43 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.035333, low=0.023525, high=0.047194, test=1.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.029423, low=0.023525, high=0.035333, test=0.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.032376, low=0.029423, high=0.035333, test=1.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.033854, low=0.032376, high=0.035333, test=1.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.033115, low=0.032376, high=0.033854, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.033485, low=0.033115, high=0.033854, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0334845251358968  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 4.34 
Iteration 31 of at most 1000 with parameter: 
     edges   triangle 
-5.1153659  0.1073755 
Sampler accepted  13.889% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
 -14022.45 -352671.59 
Average estimating equation values:
     edges   triangle 
 -14022.45 -352671.59 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.215032, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.102917, low=0.000000, high=0.215032, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.051196, low=0.000000, high=0.102917, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.025627, low=0.000000, high=0.051196, test=0.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.038407, low=0.025627, high=0.051196, test=1.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.032017, low=0.025627, high=0.038407, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.028822, low=0.025627, high=0.032017, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.027224, low=0.025627, high=0.028822, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.026426, low=0.025627, high=0.027224, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.026825, low=0.026426, high=0.027224, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0268250273378851  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 3.655 
Iteration 32 of at most 1000 with parameter: 
     edges   triangle 
-5.0922442  0.1072289 
Sampler accepted  12.962% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
       edges     triangle 
    -44.2651 1761285.1957 
Average estimating equation values:
       edges     triangle 
    -44.2651 1761285.1957 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.212389, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.101381, low=0.000000, high=0.212389, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.050352, low=0.000000, high=0.101381, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.025182, low=0.000000, high=0.050352, test=0.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.037758, low=0.025182, high=0.050352, test=1.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.031469, low=0.025182, high=0.037758, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.028325, low=0.025182, high=0.031469, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.026754, low=0.025182, high=0.028325, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.025968, low=0.025182, high=0.026754, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.026361, low=0.025968, high=0.026754, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0263607013710758  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 1.38 
Iteration 33 of at most 1000 with parameter: 
     edges   triangle 
-5.0829677  0.1071697 
Sampler accepted  15.053% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
     edges   triangle 
  247689.9 53595153.6 
Average estimating equation values:
     edges   triangle 
  247689.9 53595153.6 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.212207, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, inside hull.
iter= 2, est=0.358398, low=0.212207, high=1.000000, test=1.
is.inCH: iter= 1, inside hull.
iter= 3, est=0.475017, low=0.358398, high=1.000000, test=1.
is.inCH: iter= 1, inside hull.
iter= 4, est=0.573911, low=0.475017, high=1.000000, test=1.
is.inCH: iter= 1, inside hull.
iter= 5, est=0.660354, low=0.573911, high=1.000000, test=1.
is.inCH: iter= 1, inside hull.
iter= 6, est=0.736830, low=0.660354, high=1.000000, test=1.
is.inCH: iter= 1, inside hull.
iter= 7, est=0.804167, low=0.736830, high=1.000000, test=1.
is.inCH: iter= 1, inside hull.
iter= 8, est=0.861952, low=0.804167, high=1.000000, test=1.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.829367, low=0.804167, high=0.861952, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.844457, low=0.829367, high=0.861952, test=1.
is.inCH: iter= 1, inside hull.
iter= 11, est=0.852853, low=0.844457, high=0.861952, test=1.
is.inCH: iter= 1, outside hull.
iter= 12, est=0.848574, low=0.844457, high=0.852853, test=0.
is.inCH: iter= 1, inside hull.
iter= 13, est=0.850693, low=0.848574, high=0.852853, test=1.
is.inCH: iter= 1, outside hull.
iter= 14, est=0.849628, low=0.848574, high=0.850693, test=0.
is.inCH: iter= 1, outside hull.
iter= 15, est=0.849100, low=0.848574, high=0.849628, test=0.
is.inCH: iter= 1, inside hull.
iter= 16, est=0.849364, low=0.849100, high=0.849628, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.849363907521887  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 0.5237 
Iteration 34 of at most 1000 with parameter: 
     edges   triangle 
-5.0829839  0.1071697 
Sampler accepted   5.627% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
    edges  triangle 
  1328383 689286606 
Average estimating equation values:
    edges  triangle 
  1328383 689286606 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.731548, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.568744, low=0.000000, high=0.731548, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.444452, low=0.000000, high=0.568744, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.342121, low=0.000000, high=0.444452, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.255502, low=0.000000, high=0.342121, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.182377, low=0.000000, high=0.255502, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.122632, low=0.000000, high=0.182377, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.076894, low=0.000000, high=0.122632, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.044952, low=0.000000, high=0.076894, test=0.
is.inCH: iter= 1, inside hull.
iter= 10, est=0.062036, low=0.044952, high=0.076894, test=1.
is.inCH: iter= 1, outside hull.
iter= 11, est=0.053816, low=0.044952, high=0.062036, test=0.
is.inCH: iter= 1, outside hull.
iter= 12, est=0.049471, low=0.044952, high=0.053816, test=0.
is.inCH: iter= 1, outside hull.
iter= 13, est=0.047234, low=0.044952, high=0.049471, test=0.
is.inCH: iter= 1, inside hull.
iter= 14, est=0.048358, low=0.047234, high=0.049471, test=1.
is.inCH: iter= 1, outside hull.
iter= 15, est=0.047798, low=0.047234, high=0.048358, test=0.
is.inCH: iter= 1, inside hull.
iter= 16, est=0.048078, low=0.047798, high=0.048358, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.0480784469577391  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 2.071 
Iteration 35 of at most 1000 with parameter: 
     edges   triangle 
-5.0830954  0.1071698 
Sampler accepted   0.247% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
    edges  triangle 
  1500991 934170225 
Average estimating equation values:
    edges  triangle 
  1500991 934170225 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.220967, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.106395, low=0.000000, high=0.220967, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.053118, low=0.000000, high=0.106395, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.026643, low=0.000000, high=0.053118, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.013356, low=0.000000, high=0.026643, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.006688, low=0.000000, high=0.013356, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.003347, low=0.000000, high=0.006688, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001674, low=0.000000, high=0.003347, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000837, low=0.000000, high=0.001674, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000419, low=0.000000, high=0.000837, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000418675545390171  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood did not improve.
Iteration 36 of at most 1000 with parameter: 
    edges  triangle 
-5.518447  0.107417 
Sampler accepted   0.012% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
    edges  triangle 
  1508659 947756321 
Average estimating equation values:
    edges  triangle 
  1508659 947756321 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202299, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095595, low=0.000000, high=0.202299, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047200, low=0.000000, high=0.095595, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023528, low=0.000000, high=0.047200, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011755, low=0.000000, high=0.023528, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005877, low=0.000000, high=0.011755, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005877, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000735, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000735, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.00036727420385676  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood did not improve.
Iteration 37 of at most 1000 with parameter: 
       edges     triangle 
-92185.43414     51.64537 
Sampler accepted   0.000% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
    edges  triangle 
  1509014 948390633 
Average estimating equation values:
    edges  triangle 
  1509014 948390633 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000367224158271861  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood did not improve.
Iteration 38 of at most 1000 with parameter: 
       edges     triangle 
-92185.43414     51.64458 
Sampler accepted   0.000% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
    edges  triangle 
  1509028 948416946 
Average estimating equation values:
    edges  triangle 
  1509028 948416946 
is.inCH: iter= 1, outside hull.
iter= 1, est=0.202280, low=0.000000, high=1.000000, test=0.
is.inCH: iter= 1, outside hull.
iter= 2, est=0.095584, low=0.000000, high=0.202280, test=0.
is.inCH: iter= 1, outside hull.
iter= 3, est=0.047194, low=0.000000, high=0.095584, test=0.
is.inCH: iter= 1, outside hull.
iter= 4, est=0.023525, low=0.000000, high=0.047194, test=0.
is.inCH: iter= 1, outside hull.
iter= 5, est=0.011754, low=0.000000, high=0.023525, test=0.
is.inCH: iter= 1, outside hull.
iter= 6, est=0.005876, low=0.000000, high=0.011754, test=0.
is.inCH: iter= 1, outside hull.
iter= 7, est=0.002938, low=0.000000, high=0.005876, test=0.
is.inCH: iter= 1, outside hull.
iter= 8, est=0.001469, low=0.000000, high=0.002938, test=0.
is.inCH: iter= 1, outside hull.
iter= 9, est=0.000734, low=0.000000, high=0.001469, test=0.
is.inCH: iter= 1, outside hull.
iter= 10, est=0.000367, low=0.000000, high=0.000734, test=0.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  0.000367224109551399  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood did not improve.
Iteration 39 of at most 1000 with parameter: 
      edges    triangle 
-92185.4342     51.4947 
Sampler accepted  15.888% of 10000000 proposed steps.
Sample size = 10000 by 10000 
Back from unconstrained MCMC. Average statistics:
      edges    triangle 
   59915.77 32550967.91 
Average estimating equation values:
      edges    triangle 
   59915.77 32550967.91 
is.inCH: iter= 1, inside hull.
iter= 1, est=1.000000, low=1.000000, high=1.000000, test=1.
Calling MCMLE Optimization...
Using Newton-Raphson Step with step length  1  ...
Using lognormal metric (see control.ergm function).
Using log-normal approx (no optim)
The log-likelihood improved by 0.03247 
Step length converged once. Increasing MCMC sample size.
Iteration 40 of at most 1000 with parameter: 
      edges    triangle 
-92185.4342     51.4947 
Sampler accepted   0.000% of 40000000 proposed steps.
Sample size = 40000 by 40000 
Back from unconstrained MCMC. Average statistics:
   edges triangle 
  -95707 -9074443 
Error in ergm.MCMLE(init, nw, model, initialfit = (initialfit <- NULL),  : 
  Unconstrained MCMC sampling did not mix at all. Optimization cannot continue.
Calls: ergm -> ergm.MCMLE
Execution halted
